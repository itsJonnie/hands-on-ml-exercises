{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5uVJ9CZuiy_"
      },
      "source": [
        "# Chapter 3 Classification Exercises:\n",
        "* JONATHAN SHER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4zgMwhuiy_"
      },
      "source": [
        "### Setup:\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCTL9iFquizA"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhDZeJ3-uizA"
      },
      "source": [
        "## Exercise 1:\n",
        "\n",
        "Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the KNeighborsClassifier works quite well for this task; you just need to find good hyperparameter values (try a grid search on the weights and n_neighbors hyperparameters).\n",
        "\n",
        "* The MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LORsJy6zuizA",
        "outputId": "8c34494a-67da-416d-f445-5badaa7e1957"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import MNIST dataset and do some data exploration\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "mnist.keys()\n",
        "\n",
        "# mnist.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyCOAOW0uizB",
        "outputId": "6c2fc90b-a78e-42e4-dced-abdccb42ed7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving figure some_digit_plot\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "some_digit = X[0]\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "save_fig(\"some_digit_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Oui8OquizB"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
        "\n",
        "# Preprocess the Data:\n",
        "# The pixel values in the MNIST dataset range from 0 to 255. We'll normalize them to the range [0, 1].\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# #Step 3: Perform Grid Search for Hyperparameter Tuning\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Define the parameter grid\n",
        "# param_grid = {\n",
        "#     'n_neighbors': [3, 5, 7, 9, 11],\n",
        "#     'weights': ['uniform', 'distance']\n",
        "# }\n",
        "\n",
        "# # Initialize the KNeighborsClassifier\n",
        "# knn_clf = KNeighborsClassifier()\n",
        "\n",
        "# # Perform grid search\n",
        "# grid_search = GridSearchCV(knn_clf, param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=3)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best parameters and the corresponding accuracy\n",
        "# print(\"Best parameters:\", grid_search.best_params_)\n",
        "# print(\"Best cross-validation accuracy:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3u4Xaa_uizB",
        "outputId": "009ea328-14d0-47ec-caba-2376ffccf827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "[CV 1/3] END ....n_neighbors=3, weights=uniform;, score=0.970 total time=  23.8s\n",
            "[CV 2/3] END ....n_neighbors=3, weights=uniform;, score=0.971 total time=  24.8s\n",
            "[CV 3/3] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  26.5s\n",
            "[CV 1/3] END ...n_neighbors=3, weights=distance;, score=0.971 total time=  24.9s\n",
            "[CV 2/3] END ...n_neighbors=3, weights=distance;, score=0.973 total time=  25.3s\n",
            "[CV 3/3] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  25.2s\n",
            "[CV 1/3] END ....n_neighbors=4, weights=uniform;, score=0.968 total time=  28.7s\n",
            "[CV 2/3] END ....n_neighbors=4, weights=uniform;, score=0.970 total time=  29.1s\n",
            "[CV 3/3] END ....n_neighbors=4, weights=uniform;, score=0.967 total time=  28.1s\n",
            "[CV 1/3] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  28.3s\n",
            "[CV 2/3] END ...n_neighbors=4, weights=distance;, score=0.974 total time=  28.4s\n",
            "[CV 3/3] END ...n_neighbors=4, weights=distance;, score=0.970 total time=  28.0s\n",
            "[CV 1/3] END ....n_neighbors=5, weights=uniform;, score=0.967 total time=  35.6s\n",
            "[CV 2/3] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  34.2s\n",
            "[CV 3/3] END ....n_neighbors=5, weights=uniform;, score=0.967 total time=  29.2s\n",
            "[CV 1/3] END ...n_neighbors=5, weights=distance;, score=0.969 total time=  28.8s\n",
            "[CV 2/3] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  29.3s\n",
            "[CV 3/3] END ...n_neighbors=5, weights=distance;, score=0.968 total time=  29.0s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, estimator=KNeighborsClassifier(),\n",
              "             param_grid=[{'n_neighbors': [3, 4, 5],\n",
              "                          'weights': ['uniform', 'distance']}],\n",
              "             verbose=3)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "\n",
        "# Initialize the KNeighborsClassifier\n",
        "knn_clf = KNeighborsClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=3, verbose=3)\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E60U4puuizB",
        "outputId": "f0b4d10e-332c-4f1e-99f0-9d716ffcd157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'n_neighbors': 4, 'weights': 'distance'}\n",
            "Best cross-validation accuracy: 0.9720000000000001\n"
          ]
        }
      ],
      "source": [
        "# Print the best parameters and the corresponding accuracy\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVeD8tIPuizB"
      },
      "source": [
        "* Evaluate the Best Model on the Test Set\n",
        "* Once we have the best hyperparameters, we can evaluate the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paCwIp1VuizB",
        "outputId": "2d146737-6441-466a-dbc9-a214ba84ea76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.9729\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get the best model\n",
        "best_knn_clf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = best_knn_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test set accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF_rM8gyuizB"
      },
      "source": [
        "Our goal of achieving over 97% accuracy on the test set has been achieved with a test set accuracy of 97.29%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "* Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.5 Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called data augmentation or training set expansion."
      ],
      "metadata": {
        "id": "qFF0Vvs766pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Function to shift an image by one pixel in a specified direction\n",
        "def shift_image(image, direction):\n",
        "    \"\"\"\n",
        "    Shift a 2D image by one pixel in the specified direction.\n",
        "\n",
        "    Parameters:\n",
        "      - image: a 2D numpy array (e.g., shape (28,28) for MNIST)\n",
        "      - direction: one of 'left', 'right', 'up', 'down'\n",
        "\n",
        "    Returns:\n",
        "      - shifted: a 2D numpy array with the image shifted and the new area filled with zeros.\n",
        "    \"\"\"\n",
        "    shifted = np.copy(image)\n",
        "    if direction == 'left':\n",
        "        shifted = np.roll(shifted, shift=-1, axis=1)\n",
        "        shifted[:, -1] = 0\n",
        "    elif direction == 'right':\n",
        "        shifted = np.roll(shifted, shift=1, axis=1)\n",
        "        shifted[:, 0] = 0\n",
        "    elif direction == 'up':\n",
        "        shifted = np.roll(shifted, shift=-1, axis=0)\n",
        "        shifted[-1, :] = 0\n",
        "    elif direction == 'down':\n",
        "        shifted = np.roll(shifted, shift=1, axis=0)\n",
        "        shifted[0, :] = 0\n",
        "    else:\n",
        "        raise ValueError(\"Direction must be 'left', 'right', 'up', or 'down'.\")\n",
        "    return shifted\n",
        "\n",
        "# 2. Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 3. Create augmented training set: For each image, create 4 shifted copies (left, right, up, down)\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for image, label in zip(x_train, y_train):\n",
        "    # Original image\n",
        "    augmented_images.append(image)\n",
        "    augmented_labels.append(label)\n",
        "    # Four shifted copies\n",
        "    for direction in ['left', 'right', 'up', 'down']:\n",
        "        shifted_img = shift_image(image, direction)\n",
        "        augmented_images.append(shifted_img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "x_train_aug = np.array(augmented_images)\n",
        "y_train_aug = np.array(augmented_labels)\n",
        "\n",
        "# 4. Preprocess the data\n",
        "x_train_aug = x_train_aug.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape to include channel dimension (for CNN input)\n",
        "x_train_aug = np.expand_dims(x_train_aug, -1)  # shape becomes (num_samples, 28, 28, 1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_cat = to_categorical(y_train_aug, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 5. Build a simple CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. Train the model on the augmented training set\n",
        "history = model.fit(x_train_aug, y_train_cat, epochs=5, batch_size=128, validation_data=(x_test, y_test_cat))\n",
        "\n",
        "# 7. Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "dYuMlLmb7slw",
        "outputId": "55f9fad7-5fdb-4376-af2c-e2085a99a54d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 60ms/step - accuracy: 0.9337 - loss: 0.2261 - val_accuracy: 0.9886 - val_loss: 0.0347\n",
            "Epoch 2/5\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 60ms/step - accuracy: 0.9893 - loss: 0.0349 - val_accuracy: 0.9906 - val_loss: 0.0299\n",
            "Epoch 3/5\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 59ms/step - accuracy: 0.9940 - loss: 0.0188 - val_accuracy: 0.9905 - val_loss: 0.0295\n",
            "Epoch 4/5\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 61ms/step - accuracy: 0.9963 - loss: 0.0117 - val_accuracy: 0.9903 - val_loss: 0.0321\n",
            "Epoch 5/5\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 61ms/step - accuracy: 0.9975 - loss: 0.0081 - val_accuracy: 0.9894 - val_loss: 0.0355\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0475\n",
            "Test accuracy: 0.9894000291824341\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": false,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}